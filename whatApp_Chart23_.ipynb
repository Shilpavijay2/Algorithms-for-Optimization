{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOZzAkhrvcw3hPUDneu0ImO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shilpavijay2/Algorithms-for-Optimization/blob/main/whatApp_Chart23_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R41PoJUxAeAv"
      },
      "outputs": [],
      "source": [
        "pip install emoji"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "m3LL7bnOGVRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLTK (Natural Language Toolkit) is a popular Python library for natural language processing (NLP). It provides us various text processing libraries with a lot of test datasets.\n",
        "\n",
        "Natural language ToolKit(NLTK) is used for doing NLP tasks such as removing stopwords, tokenizing words, etc.\n",
        "\n",
        "Vader evaluates any given text and generates a positive, negative, or neutral score for each lexical feature. These scores are then added together to form a compound score, which is a matrix normalizing all scores from -1 to +1"
      ],
      "metadata": {
        "id": "ZBZPy2sSKEdz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('vader_lexicon')"
      ],
      "metadata": {
        "id": "4-GM5xgCGcY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regular Expression, is a sequence of characters that forms a search pattern. RegEx can be used to check if a string contains the specified search pattern.\n",
        "\n",
        "Pandas is a Python library used for working with data sets. It has functions for analyzing, cleaning, exploring, and manipulating data.\n",
        "\n",
        "NumPy excels in creating N-dimension data objects and performing mathematical operations efficiently, while Pandas is renowned for data wrangling and its ability to handle large datasets.\n",
        "\n",
        "Counter is an unordered collection where elements are stored as Dict keys and their count as dict value. Counter elements count can be positive, zero or negative integers. However there is no restriction on it's keys and values.\n",
        "\n",
        "plot(): To create line plots.\n",
        "scatter(): To create scatter plots.\n",
        "bar(): To create bar charts.\n",
        "hist(): To create histograms.\n",
        "show(): To display the figure.\n",
        "\n",
        "A comprehensive library for creating static, animated, and interactive visualizations in Python.\n",
        "It is widely used for data visualization in scientific computing and data analysis."
      ],
      "metadata": {
        "id": "FEcwNd1q3Epm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "generate(): Takes a string of text as input and creates the word cloud based on the frequency of words.\n",
        "to_file(): Saves the generated word cloud to an image file.\n",
        "recolor(): Recolors the words based on an image or color palette.\n",
        "\n",
        "You can customize various aspects of the word cloud, including the shape, color, font, and the words to include or exclude.\n",
        "This is a predefined set of common words (like \"the\", \"is\", \"in\", etc.) that are often filtered out in text processing because they do not carry significant meaning.\n",
        "When creating a word cloud, you might want to exclude these stop words to focus on more meaningful words.\n",
        "\n",
        "This class is used to generate colors for the words in the word cloud based on an input image."
      ],
      "metadata": {
        "id": "IiEupH2Ybwrr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import regex\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator"
      ],
      "metadata": {
        "id": "5NFn06QIGRWD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The pattern you provided is a regular expression (regex) that seems to be designed to match a specific date and time format. Let's break down the regex pattern\n",
        "\n",
        "find_author(s) takes a string s as an argument.\n",
        "String Splitting: s.split(\":\") divides the string into parts wherever a colon appears.\n",
        "\n"
      ],
      "metadata": {
        "id": "zqEiKFDizvu-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def date_time(s):\n",
        "  # Your regex pattern\n",
        "    pattern = '^([0-9]+)(\\/)([0-9]+)(\\/)([0-9]+), ([0-9]+):([0-9]+)[ ]?(AM|PM|am|pm)? -'\n",
        "    # Match the pattern against the string\n",
        "    result = regex.match(pattern, s)\n",
        "     # Check if there was a match and return True or False\n",
        "    if result:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def find_author(s):\n",
        "   # Split the string at the colon\n",
        "    s = s.split(\":\")\n",
        "    # Check if there are exactly two parts\n",
        "    if len(s)==2:\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "def getDatapoint(line):\n",
        "   # Split the line by the delimiter ' - '\n",
        "    splitline = line.split(' - ')\n",
        "    # Extract the date and time from the first part\n",
        "    dateTime = splitline[0]\n",
        "\n",
        "    date, time = dateTime.split(\", \")\n",
        "    # Combine the rest of the splitline into a message\n",
        "    message = \" \".join(splitline[1:])\n",
        "     # Check if the message contains an author\n",
        "    if find_author(message):\n",
        "        splitmessage = message.split(\": \")\n",
        "        author = splitmessage[0]\n",
        "        message = \" \".join(splitmessage[1:])\n",
        "    else:\n",
        "        author= None\n",
        "    return date, time, author, message"
      ],
      "metadata": {
        "id": "JPb8ZrPkGiPF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/shilpa"
      ],
      "metadata": {
        "id": "_34RqGiOIAeF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The with statement is used for resource management. It ensures that the file is properly closed after its suite finishes, even if an error occurs. This is generally preferred over manually opening and closing files.\n"
      ],
      "metadata": {
        "id": "SNfs7dIW9Hhf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = []\n",
        "conversation = '/content/drive/MyDrive/shilpa/chart45.txt'\n",
        "with open(conversation, encoding=\"utf-8\") as fp:\n",
        "    fp.readline() # Optionally read the header or first line # Optionally skip the header or initial information\n",
        "    messageBuffer = []\n",
        "    date, time, author = None, None, None\n",
        "    while True:\n",
        "        line = fp.readline() # Read the next line\n",
        "        if not line:  # Break if the line is empty (EOF)\n",
        "            break\n",
        "            # Process the line using your previously defined getDatapoint function\n",
        "        line = line.strip() # Clean up any leading/trailing whitespace\n",
        "        if date_time(line): # Check if this line indicates a new date/time\n",
        "            if len(messageBuffer) > 0: # If there are messages to save\n",
        "                data.append([date, time, author, ' '.join(messageBuffer)])\n",
        "            messageBuffer.clear() # Clear the buffer for new messages\n",
        "            date, time, author, message = getDatapoint(line)  # Extract new info\n",
        "            messageBuffer.append(message) # Start the new message buffer\n",
        "        else:\n",
        "            messageBuffer.append(line)  # Add to the current message"
      ],
      "metadata": {
        "id": "W-mTZrBEGwbr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(data, columns=[\"Date\", 'Time', 'Author', 'Message']) # Create a DataFrame from the processed data\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "print(df.tail(100)) # Corrected the typo from 'taiCl' to 'tail'\n",
        "print(df.info()) # Display DataFrame information\n",
        "print(df.Author.unique()) # Display unique authors"
      ],
      "metadata": {
        "id": "a-g3Ob9qIEH5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head() # Display the first few rows of the DataFrame"
      ],
      "metadata": {
        "id": "qQylFVYQgh1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_messages = df.shape[0] # Count total messages\n",
        "print(total_messages)"
      ],
      "metadata": {
        "id": "fKPYKmbNIqlH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "media_messages = df[df[\"Message\"]=='<Media omitted>'].shape[0]\n",
        "print(media_messages) # Count media messages"
      ],
      "metadata": {
        "id": "Hj40i1eCItde"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import emoji\n",
        "import regex\n",
        "\n",
        "def split_count(text):\n",
        "    emoji_list = []\n",
        "    data = regex.findall(r'\\X',text)\n",
        "    for word in data:\n",
        "        # Use emoji.is_emoji() to check for emojis\n",
        "        if any(emoji.is_emoji(char) for char in word):\n",
        "            emoji_list.append(word)\n",
        "    return emoji_list\n",
        "\n",
        "df['emoji'] = df[\"Message\"].apply(split_count) # Apply the split_count function to each message\n",
        "\n",
        "emojis = sum(df['emoji'].str.len()) # Count the total number of emojis\n",
        "print(emojis)"
      ],
      "metadata": {
        "id": "bRQvlBebzr9_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "URLPATTERN = r'(https?://\\S+)' # Define the URL pattern\n",
        "df['urlcount'] = df.Message.apply(lambda x: regex.findall(URLPATTERN, x)).str.len() # Count URLs in each message\n",
        "links = np.sum(df.urlcount) # Calculate total links\n",
        "\n",
        "print(\"Chats betweent Areej Clg and Pratiksha Awate\")\n",
        "print(\"Total Messages: \", total_messages) # Assuming total_messages and media_messages are defined previously\n",
        "print(\"Number of Media Shared: \", media_messages)\n",
        "\n",
        "print(\"Number of Links Shared\", links)"
      ],
      "metadata": {
        "id": "4pRq0MPlJqyC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "media_messages_df = df[df['Message'] == '<Media omitted>'] # Filter out media messages\n",
        "messages_df = df.drop(media_messages_df.index)\n",
        "messages_df['Letter_Count'] = messages_df['Message'].apply(lambda s : len(s)) # Calculate letter count\n",
        "messages_df['Word_Count'] = messages_df['Message'].apply(lambda s : len(s.split(' '))) # Calculate word count\n",
        "messages_df[\"MessageCount\"]=1 # Create a MessageCount column\n"
      ],
      "metadata": {
        "id": "zEOydpwBJ0Dv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pKh7eGLvUPuF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_emojis_list = list(set([a for b in messages_df.emoji for a in b]))  # Extract unique emojis\n",
        "total_emojis = len(total_emojis_list)\n",
        "\n",
        "total_emojis_list = list([a for b in messages_df.emoji for a in b]) # Count all emojis\n",
        "emoji_dict = dict(Counter(total_emojis_list))\n",
        "emoji_dict = sorted(emoji_dict.items(), key=lambda x: x[1], reverse=True)\n",
        "for i in emoji_dict:\n",
        "  print(i)\n",
        "\n",
        "emoji_df = pd.DataFrame(emoji_dict, columns=['emoji', 'count']) # Create DataFrame for emojis\n",
        "import plotly.express as px\n",
        "fig = px.pie(emoji_df, values='count', names='emoji') # Plotting with Plotly\n",
        "fig.update_traces(textposition='inside', textinfo='percent+label')\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "j1AVvBwJLmWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \" \".join(review for review in messages_df.Message) # Combine all messages into a single string\n",
        "print (\"There are {} words in all the messages.\".format(len(text))) # Print the total word count\n",
        "stopwords = set(STOPWORDS) # Define stopwords\n",
        "# Generate a word cloud image\n",
        "wordcloud = WordCloud(stopwords=stopwords, background_color=\"white\").generate(text)\n",
        "# Display the generated image:\n",
        "# the matplotlib way:\n",
        "plt.figure( figsize=(10,5))# Display the generated image using Matplotlib\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\") # Turn off the axis  # Hide the axes\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "58e0RLNtPuNl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages_df = df  # Assign the dataframe 'df' to 'messages_df' to make the 'Author' column available\n",
        "l = [\"Shilpa Dhanure\",\"Areej Clg\", \"Pratiksha Awate\",\"Monika Kharkwal\"]\n",
        "for i in range(len(l)):\n",
        "  dummy_df = messages_df[messages_df['Author'] == l[i]]\n",
        "  text = \" \".join(review for review in dummy_df.Message)\n",
        "  stopwords = set(STOPWORDS)\n",
        "  # Generate a word cloud image\n",
        "  print('Author name',l[i])\n",
        "\n",
        "  # Check if text is empty after stop word removal\n",
        "  words = [word for word in text.split() if word.lower() not in stopwords]\n",
        "  if len(words) == 0:\n",
        "    print(f\"No words found for author {l[i]} after removing stop words. Skipping word cloud generation.\")\n",
        "    continue  # Skip to next author if no words are found\n",
        "\n",
        "  wordcloud = WordCloud(stopwords=stopwords, background_color=\"white\").generate(\" \".join(words))\n",
        "  # Display the generated image\n",
        "  plt.figure( figsize=(10,5))\n",
        "  plt.imshow(wordcloud, interpolation='bilinear')\n",
        "  plt.axis(\"off\")\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "v1O7r23bQNkG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}